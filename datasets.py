# datasets.py, generated by ChatGPT 5.2 Auto
import os
import json
from datasets import load_dataset

def save_as_json_array(ds_split, out_path: str, keep_cols=("text", "label")):
    """
    Save a HuggingFace Dataset split as a single JSON array (NOT JSONL),
    keeping only the columns needed by main.py.
    """
    # Convert to pandas, keep only required columns, then to list-of-dicts
    df = ds_split.to_pandas()
    missing = [c for c in keep_cols if c not in df.columns]
    if missing:
        raise ValueError(f"Missing required columns {missing}. Available columns: {list(df.columns)}")

    records = df[list(keep_cols)].to_dict(orient="records")

    # Write a single JSON array
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(records, f, ensure_ascii=False)

def main():
    os.makedirs("data", exist_ok=True)

    # Use the dataset specified by the assignment option you're running
    ds = load_dataset("yaful/MAGE")

    save_as_json_array(ds["train"],      os.path.join("data", "TrainingData.json"))
    save_as_json_array(ds["validation"], os.path.join("data", "ValidationData.json"))
    save_as_json_array(ds["test"],       os.path.join("data", "TestingData.json"))

    # Quick sanity check: files should start with '[' (JSON array), not '{' (JSONL line)
    for fn in ["TrainingData.json", "ValidationData.json", "TestingData.json"]:
        path = os.path.join("data", fn)
        with open(path, "r", encoding="utf-8") as f:
            first_char = f.read(1)
        if first_char != "[":
            raise RuntimeError(f"{fn} does not look like a JSON array. First char: {first_char!r}")

    print("Wrote JSON array files to ./data/ (TrainingData.json, ValidationData.json, TestingData.json)")

if __name__ == "__main__":
    main()